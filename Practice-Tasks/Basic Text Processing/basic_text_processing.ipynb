{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZUug3pnw3JV"
      },
      "source": [
        "**TOKENIZATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YU5rShTwws4P",
        "outputId": "4c6bb091-05e6-40be-eb81-142af59d5ece"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentences: ['Natural Language Processing (NLP) is an interesting field.', 'It combines AI and linguistics.']\n",
            "Words: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'an', 'interesting', 'field', '.', 'It', 'combines', 'AI', 'and', 'linguistics', '.']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"Natural Language Processing (NLP) is an interesting field. It combines AI and linguistics.\"\n",
        "sentences = sent_tokenize(text)  # Sentence tokenization\n",
        "words = word_tokenize(text)      # Word tokenization\n",
        "\n",
        "print(\"Sentences:\", sentences)\n",
        "print(\"Words:\", words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwpufoN-w9CL"
      },
      "source": [
        "**STEMMING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYANAvpxwHC3",
        "outputId": "27bfe45b-5a6a-4999-dd11-60dae9008768"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stemmed Words: ['the', 'cat', 'are', 'run', 'and', 'eat', 'their', 'meal', '.', 'automat']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "ps = PorterStemmer()\n",
        "words = word_tokenize(\"The cats are running and eating their meals. automatically\")\n",
        "stemmed_words = [ps.stem(word) for word in words]\n",
        "\n",
        "print(\"Stemmed Words:\", stemmed_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lmyTzPuxAfg"
      },
      "source": [
        "**LEMMATIZATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6Er_9T_wM_L",
        "outputId": "2b1479e4-03fc-4181-beaa-2f18518a6fc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lemmatized Words: ['The', 'boys', 'be', 'run', 'and', 'eat', 'their', 'food', '.', 'automatically']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = word_tokenize(\"The boys are running and ate their food. automatically\")\n",
        "\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]  # 'v' indicates verb\n",
        "\n",
        "print(\"Lemmatized Words:\", lemmatized_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aiu5LBqcwPHI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
